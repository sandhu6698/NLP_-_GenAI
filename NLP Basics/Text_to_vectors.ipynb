{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Vectorization is the process of converting text data into numerical data that machine learning models can understand. In the context of natural language processing (NLP), vectorization typically involves transforming text into vectors of numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "sample_sentences = [\n",
    "    \"I love programming.\",  # First sample sentence\n",
    "    \"The sun is shining today.\",  # Second sample sentence\n",
    "    \"She sings beautifully.\",  # Third sample sentence\n",
    "    \"The cat is sleeping.\",  # Fourth sample sentence\n",
    "    \"He enjoys playing video games.\"  # Fifth sample sentence\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['love', 'programming', '.'], ['sun', 'shining', 'today', '.'], ['sings', 'beautifully', '.'], ['cat', 'sleeping', '.'], ['enjoys', 'playing', 'video', 'game', '.']]\n",
      "[['love', 'programming', '.'], ['sun', 'shining', 'today', '.'], ['sings', 'beautifully', '.'], ['cat', 'sleeping', '.'], ['enjoys', 'playing', 'video', 'game', '.']]\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import ne_chunk\n",
    "import string\n",
    "\n",
    "#Text Preprocessing 1\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to word tokenize a sentence\n",
    "def word_tokenize_sentence(sentence):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    return tokens\n",
    "\n",
    "# Tokenize and lemmatize each sentence\n",
    "lemmatized_sentences = []\n",
    "for sentence in sample_sentences:\n",
    "    tokens = word_tokenize_sentence(sentence)\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    lemmatized_sentences.append(lemmatized_tokens)\n",
    "\n",
    "# Remove stop words from lemmatized_sentences\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_sentences = []\n",
    "for sentence in lemmatized_sentences:\n",
    "    filtered_tokens = [token for token in sentence if token.lower() not in stop_words]\n",
    "    filtered_sentences.append(filtered_tokens)\n",
    "\n",
    "# Print the filtered sentences\n",
    "print(filtered_sentences)\n",
    "# Function to remove named entities from a sentence\n",
    "def remove_named_entities(sentence):\n",
    "    tokens = word_tokenize_sentence(sentence)\n",
    "    tagged_tokens = nltk.pos_tag(tokens)\n",
    "    named_entities = ne_chunk(tagged_tokens, binary=True)\n",
    "    filtered_tokens = [token for token, pos in named_entities if pos != 'NE']\n",
    "    return filtered_tokens\n",
    "\n",
    "# Remove named entities from each sentence in filtered_sentences\n",
    "filtered_sentences_without_entities = []\n",
    "for sentence in filtered_sentences:\n",
    "    filtered_tokens = remove_named_entities(' '.join(sentence))\n",
    "    filtered_sentences_without_entities.append(filtered_tokens)\n",
    "\n",
    "# Print the filtered sentences without named entities\n",
    "print(filtered_sentences_without_entities)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['love', 'programming'], ['sun', 'shining', 'today'], ['sings', 'beautifully'], ['cat', 'sleeping'], ['enjoys', 'playing', 'video', 'game']]\n"
     ]
    }
   ],
   "source": [
    "# Remove punctuations from each sentence in filtered_sentences_without_entities\n",
    "filtered_sentences_without_punctuation = []\n",
    "for sentence in filtered_sentences_without_entities:\n",
    "    sentence_without_punctuation = ' '.join(sentence).translate(str.maketrans('', '', string.punctuation)).split()\n",
    "    # sentence_without_punctuation = ' '.join(sentence).translate(str.maketrans('', '', punctuation)).split()\n",
    "    filtered_sentences_without_punctuation.append(sentence_without_punctuation)\n",
    "\n",
    "# Print the filtered sentences without punctuations\n",
    "print(filtered_sentences_without_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Convert filtered_sentences_without_punctuation to a 2D array\n",
    "filtered_sentences_2d = []\n",
    "for sentence in filtered_sentences_without_punctuation:\n",
    "    for word in sentence:\n",
    "        filtered_sentences_2d.append([word])\n",
    "\n",
    "# Create an instance of the OneHotEncoder\n",
    "encoder = OneHotEncoder()\n",
    "\n",
    "# Fit and transform the encoder on the filtered_sentences_2d\n",
    "encoded_sentences = encoder.fit_transform(filtered_sentences_2d).toarray()\n",
    "\n",
    "# Print the encoded sentences\n",
    "\n",
    "print(encoded_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 1 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 1 1 0]\n",
      " [1 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 1 0 0 0]\n",
      " [0 0 1 1 0 1 0 0 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create an instance of CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the vectorizer on the filtered_sentences_without_punctuation\n",
    "count_vectors = vectorizer.fit_transform([' '.join(sentence) for sentence in filtered_sentences_without_punctuation])\n",
    "\n",
    "# Print the count vectors\n",
    "print(count_vectors.toarray())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
