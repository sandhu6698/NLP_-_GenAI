{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Tokenization in Natural Language Processing (NLP) is the process of breaking down text into smaller units called tokens. These tokens can be words, phrases, or even characters. Tokenization is a crucial step in text preprocessing as it allows for the analysis and manipulation of text data at a granular level. It helps in various NLP tasks such as text analysis, machine learning, and information retrieval by converting the text into a structured format that can be easily processed by algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# NLTK (Natural Language Toolkit) is a Python library that provides tools and resources for working with human language data. \n",
    "# It offers a wide range of functionalities for tasks such as tokenization, stemming, tagging, parsing, semantic reasoning, and more. \n",
    "# NLTK is widely used in natural language processing (NLP) research and applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to word tokenize a sentence\n",
    "def word_tokenize_sentence(sentence):\n",
    "    \"\"\"\n",
    "    Tokenizes a sentence into individual words using the nltk library.\n",
    "\n",
    "    Parameters:\n",
    "    sentence (str): The input sentence to be tokenized.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of tokens representing the individual words in the sentence.\n",
    "    \"\"\"\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'love', 'programming', '.']\n",
      "['The', 'sun', 'is', 'shining', 'today', '.']\n",
      "['She', 'sings', 'beautifully', '.']\n",
      "['The', 'cat', 'is', 'sleeping', '.']\n",
      "['He', 'enjoys', 'playing', 'video', 'games', '.']\n"
     ]
    }
   ],
   "source": [
    "sample_sentences = [\n",
    "    \"I love programming.\",  # First sample sentence\n",
    "    \"The sun is shining today.\",  # Second sample sentence\n",
    "    \"She sings beautifully.\",  # Third sample sentence\n",
    "    \"The cat is sleeping.\",  # Fourth sample sentence\n",
    "    \"He enjoys playing video games.\"  # Fifth sample sentence\n",
    "]\n",
    "for sentence in sample_sentences:\n",
    "    tokens = word_tokenize_sentence(sentence)\n",
    "    print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Stemming in Natural Language Processing (NLP) is the process of reducing words to their base or root form, known as the stem. It involves removing suffixes from words to obtain the core meaning or essence of the word. Stemming is commonly used in text preprocessing tasks to normalize words and reduce the dimensionality of text data. By reducing words to their stems, stemming helps in tasks such as information retrieval, text classification, and sentiment analysis by treating different forms of the same word as a single entity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming example \n",
    "# [going, gone, goes] ---> go\n",
    "\n",
    "example_words = ['running', 'jumps', 'swimming', 'played', 'eating', 'happily', 'quickly', 'beautifully', 'friendly', 'lovely']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running -----> run\n",
      "jumps -----> jump\n",
      "swimming -----> swim\n",
      "played -----> play\n",
      "eating -----> eat\n",
      "happily -----> happili\n",
      "quickly -----> quickli\n",
      "beautifully -----> beauti\n",
      "friendly -----> friendli\n",
      "lovely -----> love\n"
     ]
    }
   ],
   "source": [
    "# PorterStemmer \n",
    "#The Porter Stemmer is an algorithm used in NLP to reduce words to their base or root form by applying a series of transformation rules.\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "porter_stemmer = PorterStemmer()\n",
    "for word in example_words:\n",
    "    print(f\"{word} -----> {porter_stemmer.stem(word)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'congratul'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Porter Stemmer has a disadvantage, it changes form for some of the words, below is the example\n",
    "porter_stemmer.stem(\"congratulations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running -----> runn\n",
      "jumps -----> jump\n",
      "swimming -----> swimm\n",
      "played -----> play\n",
      "eating -----> eat\n",
      "happily -----> happi\n",
      "quickly -----> quick\n",
      "beautifully -----> beautiful\n",
      "friendly -----> friend\n",
      "lovely -----> love\n"
     ]
    }
   ],
   "source": [
    "#The RegexpStemmer is a class in the nltk.stem module that uses regular expressions to remove affixes from words, effectively reducing them to their root forms based on specified patterns.\n",
    "from nltk.stem import RegexpStemmer\n",
    "\n",
    "# Define a regular expression pattern for stemming, it will remove the last characters\n",
    "pattern = 'ing$|s$|ed$|ly$'\n",
    "\n",
    "# Initialize the RegexpStemmer with the pattern\n",
    "regexp_stemmer = RegexpStemmer(pattern)\n",
    "\n",
    "for word in example_words:\n",
    "    print(f\"{word} -----> {regexp_stemmer.stem(word)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running -----> run\n",
      "jumps -----> jump\n",
      "swimming -----> swim\n",
      "played -----> play\n",
      "eating -----> eat\n",
      "happily -----> happili\n",
      "quickly -----> quick\n",
      "beautifully -----> beauti\n",
      "friendly -----> friend\n",
      "lovely -----> love\n"
     ]
    }
   ],
   "source": [
    "#The Snowball Stemmer is an algorithm used in natural language processing (NLP) for reducing words to their base or root form.\n",
    "#  It is an improvement over the Porter Stemmer and supports multiple languages.\n",
    "#  The Snowball Stemmer is known for its balance between performance and accuracy.\n",
    "\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# Initialize the SnowballStemmer for English\n",
    "snowball_stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "for word in example_words:\n",
    "    print(f\"{word} -----> {snowball_stemmer.stem(word)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Lemmatization is the process of reducing words to their base or dictionary form, known as a lemma (root word), by considering the context and morphological analysis of the words. Unlike stemming, which simply cuts off prefixes or suffixes, lemmatization uses vocabulary and morphological analysis to return the base form of a word, ensuring that the resulting lemma is a valid word in the language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VERBS SET\n",
      "running -----> run\n",
      "jumps -----> jump\n",
      "swimming -----> swim\n",
      "played -----> play\n",
      "eating -----> eat\n",
      "happily -----> happily\n",
      "quickly -----> quickly\n",
      "beautifully -----> beautifully\n",
      "friendly -----> friendly\n",
      "lovely -----> lovely\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Initialize the WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# Apply lemmatization to each word in the list\n",
    "print(\"VERBS SET\")\n",
    "for word in example_words:\n",
    "    print(f\"{word} -----> {lemmatizer.lemmatize(word, pos=wordnet.VERB)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOUNS SET\n",
      "running -----> running\n",
      "jumps -----> jump\n",
      "swimming -----> swimming\n",
      "played -----> played\n",
      "eating -----> eating\n",
      "happily -----> happily\n",
      "quickly -----> quickly\n",
      "beautifully -----> beautifully\n",
      "friendly -----> friendly\n",
      "lovely -----> lovely\n"
     ]
    }
   ],
   "source": [
    "print(\"NOUNS SET\")\n",
    "\n",
    "for word in example_words:\n",
    "    print(f\"{word} -----> {lemmatizer.lemmatize(word, pos=wordnet.NOUN)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "StopWords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Stopwords in Natural Language Processing (NLP) are common words that are often filtered out before processing text data. These words, such as \"and\", \"the\", \"is\", \"in\", and \"at\", typically do not carry significant meaning and are removed to focus on the more important words in the text. Removing stopwords helps in reducing the dimensionality of the data and improving the performance of NLP tasks like text classification, sentiment analysis, and information retrieval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1947', 'partition', 'Punjab', 'significant', 'traumatic', 'event', 'history', 'Indian', 'subcontinent', '.', 'marked', 'division', 'British', 'India', 'two', 'independent', 'dominions', ',', 'India', 'Pakistan', '.', 'Punjab', ',', 'region', 'rich', 'cultural', 'historical', 'heritage', ',', 'split', 'East', 'Punjab', ',', 'became', 'part', 'India', ',', 'West', 'Punjab', ',', 'became', 'part', 'Pakistan', '.', 'partition', 'led', 'one', 'largest', 'mass', 'migrations', 'human', 'history', ',', 'millions', 'people', 'crossing', 'borders', 'join', 'chosen', 'nation', '.', 'partition', 'accompanied', 'widespread', 'violence', ',', 'communal', 'riots', ',', 'humanitarian', 'crisis', ',', 'resulting', 'loss', 'countless', 'lives', 'displacement', 'millions', '.', 'legacy', 'partition', 'continues', 'influence', 'socio-political', 'landscape', 'India', 'Pakistan', 'day', '.']\n"
     ]
    }
   ],
   "source": [
    "paragraph = \"\"\"\n",
    "The 1947 partition of Punjab was a significant and traumatic event in the history of the Indian subcontinent. \n",
    "It marked the division of British India into two independent dominions, India and Pakistan. \n",
    "Punjab, a region with a rich cultural and historical heritage, was split into East Punjab, which became part of India, and West Punjab, which became part of Pakistan.\n",
    "This partition led to one of the largest mass migrations in human history, with millions of people crossing borders to join their chosen nation. \n",
    "The partition was accompanied by widespread violence, communal riots, and a humanitarian crisis, resulting in the loss of countless lives and the displacement of millions. \n",
    "The legacy of the partition continues to influence the socio-political landscape of both India and Pakistan to this day.\n",
    "\"\"\"\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "# Tokenize the paragraph\n",
    "words = word_tokenize(paragraph)\n",
    "\n",
    "# Get the list of stopwords for English\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Filter out the stopwords\n",
    "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "\n",
    "# Print the filtered words\n",
    "print(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('1947', 'CD'), ('partition', 'NN'), ('Punjab', 'NNP'), ('significant', 'JJ'), ('traumatic', 'JJ'), ('event', 'NN'), ('history', 'NN'), ('Indian', 'JJ'), ('subcontinent', 'NN'), ('.', '.'), ('marked', 'VBN'), ('division', 'NN'), ('British', 'JJ'), ('India', 'NNP'), ('two', 'CD'), ('independent', 'JJ'), ('dominions', 'NNS'), (',', ','), ('India', 'NNP'), ('Pakistan', 'NNP'), ('.', '.'), ('Punjab', 'NNP'), (',', ','), ('region', 'NN'), ('rich', 'JJ'), ('cultural', 'JJ'), ('historical', 'JJ'), ('heritage', 'NN'), (',', ','), ('split', 'VBD'), ('East', 'NNP'), ('Punjab', 'NNP'), (',', ','), ('became', 'VBD'), ('part', 'NN'), ('India', 'NNP'), (',', ','), ('West', 'NNP'), ('Punjab', 'NNP'), (',', ','), ('became', 'VBD'), ('part', 'NN'), ('Pakistan', 'NNP'), ('.', '.'), ('partition', 'NN'), ('led', 'VBD'), ('one', 'CD'), ('largest', 'JJS'), ('mass', 'NN'), ('migrations', 'NNS'), ('human', 'JJ'), ('history', 'NN'), (',', ','), ('millions', 'NNS'), ('people', 'NNS'), ('crossing', 'VBG'), ('borders', 'NNS'), ('join', 'VBP'), ('chosen', 'VBN'), ('nation', 'NN'), ('.', '.'), ('partition', 'NN'), ('accompanied', 'VBD'), ('widespread', 'JJ'), ('violence', 'NN'), (',', ','), ('communal', 'JJ'), ('riots', 'NNS'), (',', ','), ('humanitarian', 'JJ'), ('crisis', 'NN'), (',', ','), ('resulting', 'VBG'), ('loss', 'NN'), ('countless', 'NN'), ('lives', 'VBZ'), ('displacement', 'JJ'), ('millions', 'NNS'), ('.', '.'), ('legacy', 'NN'), ('partition', 'NN'), ('continues', 'VBZ'), ('influence', 'VB'), ('socio-political', 'JJ'), ('landscape', 'NN'), ('India', 'NNP'), ('Pakistan', 'NNP'), ('day', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  The/DT\n",
      "  1947/CD\n",
      "  partition/NN\n",
      "  of/IN\n",
      "  (PERSON Punjab/NNP)\n",
      "  was/VBD\n",
      "  a/DT\n",
      "  significant/JJ\n",
      "  and/CC\n",
      "  traumatic/JJ\n",
      "  event/NN\n",
      "  in/IN\n",
      "  the/DT\n",
      "  history/NN\n",
      "  of/IN\n",
      "  the/DT\n",
      "  (GPE Indian/JJ)\n",
      "  subcontinent/NN\n",
      "  ./.)\n",
      "(S\n",
      "  It/PRP\n",
      "  marked/VBD\n",
      "  the/DT\n",
      "  division/NN\n",
      "  of/IN\n",
      "  (GPE British/JJ)\n",
      "  India/NNP\n",
      "  into/IN\n",
      "  two/CD\n",
      "  independent/JJ\n",
      "  dominions/NNS\n",
      "  ,/,\n",
      "  (GPE India/NNP)\n",
      "  and/CC\n",
      "  (GPE Pakistan/NNP)\n",
      "  ./.)\n",
      "(S\n",
      "  (GPE Punjab/NNP)\n",
      "  ,/,\n",
      "  a/DT\n",
      "  region/NN\n",
      "  with/IN\n",
      "  a/DT\n",
      "  rich/JJ\n",
      "  cultural/JJ\n",
      "  and/CC\n",
      "  historical/JJ\n",
      "  heritage/NN\n",
      "  ,/,\n",
      "  was/VBD\n",
      "  split/VBN\n",
      "  into/IN\n",
      "  (GPE East/NNP Punjab/NNP)\n",
      "  ,/,\n",
      "  which/WDT\n",
      "  became/VBD\n",
      "  part/NN\n",
      "  of/IN\n",
      "  (GPE India/NNP)\n",
      "  ,/,\n",
      "  and/CC\n",
      "  (LOCATION West/NNP Punjab/NNP)\n",
      "  ,/,\n",
      "  which/WDT\n",
      "  became/VBD\n",
      "  part/NN\n",
      "  of/IN\n",
      "  (GPE Pakistan/NNP)\n",
      "  ./.)\n",
      "(S\n",
      "  This/DT\n",
      "  partition/NN\n",
      "  led/VBD\n",
      "  to/TO\n",
      "  one/CD\n",
      "  of/IN\n",
      "  the/DT\n",
      "  largest/JJS\n",
      "  mass/NN\n",
      "  migrations/NNS\n",
      "  in/IN\n",
      "  human/JJ\n",
      "  history/NN\n",
      "  ,/,\n",
      "  with/IN\n",
      "  millions/NNS\n",
      "  of/IN\n",
      "  people/NNS\n",
      "  crossing/VBG\n",
      "  borders/NNS\n",
      "  to/TO\n",
      "  join/VB\n",
      "  their/PRP$\n",
      "  chosen/JJ\n",
      "  nation/NN\n",
      "  ./.)\n",
      "(S\n",
      "  The/DT\n",
      "  partition/NN\n",
      "  was/VBD\n",
      "  accompanied/VBN\n",
      "  by/IN\n",
      "  widespread/JJ\n",
      "  violence/NN\n",
      "  ,/,\n",
      "  communal/JJ\n",
      "  riots/NNS\n",
      "  ,/,\n",
      "  and/CC\n",
      "  a/DT\n",
      "  humanitarian/JJ\n",
      "  crisis/NN\n",
      "  ,/,\n",
      "  resulting/VBG\n",
      "  in/IN\n",
      "  the/DT\n",
      "  loss/NN\n",
      "  of/IN\n",
      "  countless/JJ\n",
      "  lives/NNS\n",
      "  and/CC\n",
      "  the/DT\n",
      "  displacement/NN\n",
      "  of/IN\n",
      "  millions/NNS\n",
      "  ./.)\n",
      "(S\n",
      "  The/DT\n",
      "  legacy/NN\n",
      "  of/IN\n",
      "  the/DT\n",
      "  partition/NN\n",
      "  continues/VBZ\n",
      "  to/TO\n",
      "  influence/VB\n",
      "  the/DT\n",
      "  socio-political/JJ\n",
      "  landscape/NN\n",
      "  of/IN\n",
      "  both/DT\n",
      "  (GPE India/NNP)\n",
      "  and/CC\n",
      "  (GPE Pakistan/NNP)\n",
      "  to/TO\n",
      "  this/DT\n",
      "  day/NN\n",
      "  ./.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\sandh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\sandh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# To implement Named Entity Recognition (NER) using the nltk library in Python, you can follow these steps:\n",
    "\n",
    "# Tokenize the text into sentences.\n",
    "# Tokenize each sentence into words.\n",
    "# Tag each word with its part of speech.\n",
    "# Use a named entity chunker to identify named entities.\n",
    "# Here's an example of how to do this:\n",
    "# parts of speech tagging\n",
    "from nltk import pos_tag\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk import pos_tag, ne_chunk\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "# Tokenize the text into sentences\n",
    "sentences = sent_tokenize(paragraph)\n",
    "\n",
    "# Tokenize each sentence into words and perform POS tagging\n",
    "\n",
    "tokenized_sentences = [word_tokenize(sentence) for sentence in sentences]\n",
    "pos_tagged_sentences = [pos_tag(sentence) for sentence in tokenized_sentences]\n",
    "\n",
    "# Perform Named Entity Recognition\n",
    "named_entities = [ne_chunk(sentence) for sentence in pos_tagged_sentences]\n",
    "\n",
    "# Print the named entities\n",
    "for tree in named_entities:\n",
    "    print(tree)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
